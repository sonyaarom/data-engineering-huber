name: Data Processor

on:
  # Run after sitemap crawler is completed
  workflow_run:
    workflows: ["Sitemap Crawler and Content Ingestion"]
    types:
      - completed
  # Allow manual triggering
  workflow_dispatch:

permissions:
  contents: write
  
jobs:
  check-and-process:
    runs-on: ubuntu-latest
    env:
      PYTHONPATH: ${{ github.workspace }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
            path: ~/.cache/pip
            key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
            restore-keys: |
              ${{ runner.os }}-pip-
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install spacy
          python -m spacy download en_core_web_sm
              
      - name: Check for new or updated data
        id: data_check
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USERNAME: ${{ secrets.DB_USERNAME }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          TOGETHER_API_KEY: ${{ secrets.TOGETHER_API_KEY }}
        run: python scripts/data_check.py
      
      - name: Process keywords
        if: steps.data_check.outputs.processing_needed == 'true'
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USERNAME: ${{ secrets.DB_USERNAME }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          # --- FIX IS HERE ---
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          TOGETHER_API_KEY: ${{ secrets.TOGETHER_API_KEY }}
        run: |
          echo "::group::Keywords Processing Log"
          python -m hubert.data_ingestion.processors.keyword_processor
          echo "::endgroup::"
      
      - name: Process embeddings
        if: steps.data_check.outputs.processing_needed == 'true'
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USERNAME: ${{ secrets.DB_USERNAME }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          # --- AND FIX IS HERE ---
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          TOGETHER_API_KEY: ${{ secrets.TOGETHER_API_KEY }}
        run: |
          echo "::group::Embeddings Processing Log"
          python -m hubert.data_ingestion.processors.vector_processor --table-name page_embeddings_a
          echo "::endgroup::"

      - name: Create job summary
        run: |
          echo "## Data Processing Summary" >> $GITHUB_STEP_SUMMARY
          echo "Run completed at: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "data_check_results.json" ]; then
            KEYWORDS_NEEDED=$(cat data_check_results.json | jq -r '.keywords_needed // 0')
            EMBEDDINGS_NEEDED=$(cat data_check_results.json | jq -r '.embeddings_needed // 0')
            
            echo "### Data Check Results" >> $GITHUB_STEP_SUMMARY
            echo "- **Records needing keyword processing:** $KEYWORDS_NEEDED" >> $GITHUB_STEP_SUMMARY
            echo "- **Records needing embeddings processing:** $EMBEDDINGS_NEEDED" >> $GITHUB_STEP_SUMMARY
            
            if [ "${{ steps.data_check.outputs.processing_needed }}" == "true" ]; then
              echo "- **Processing status:** Completed" >> $GITHUB_STEP_SUMMARY
            else
              echo "- **Processing status:** Skipped (no new data)" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "### Data Check Results" >> $GITHUB_STEP_SUMMARY
            echo "- **Status:** Error running data check" >> $GITHUB_STEP_SUMMARY
          fi
          
      - name: Save results to repository
        if: steps.data_check.outputs.processing_needed == 'true'
        run: |
          mkdir -p metrics/$(date +"%Y-%m-%d")/processors
          
          echo "{
            \"date\": \"$(date +"%Y-%m-%d")\",
            \"keywords_processed\": \"${{ steps.data_check.outputs.keywords_needed }}\",
            \"embeddings_processed\": \"${{ steps.data_check.outputs.embeddings_needed }}\",
            \"status\": \"completed\"
          }" > metrics/$(date +"%Y-%m-%d")/processors/summary.json
          
          if [ -f "data_check_results.json" ]; then
            cp data_check_results.json metrics/$(date +"%Y-%m-%d")/processors/
          fi

      - name: Commit results
        if: steps.data_check.outputs.processing_needed == 'true'
        run: |
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'actions@github.com'
          git add metrics/
          git diff --quiet && git diff --staged --quiet || (git commit -m "Update processor metrics [skip ci]" && git push)