name: 'Sitemap Crawler and Content Ingestion'

on:
  schedule:
    # Run daily at 3:00 AM UTC
    - cron: '0 3 * * *'
  # Allow manual triggering
  workflow_dispatch:

# Grant permissions for the job to write to the repository (for committing metrics)
permissions:
  contents: write

jobs:
  crawl-and-ingest:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip' # Enable caching for pip dependencies
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # The main requirements.txt should include all necessary packages.
          # If not, add them here, e.g., pip install psycopg2-binary requests beautifulsoup4

      - name: Run Crawler and Ingestion Script
        id: crawler
        env:
          # Pass secrets to the Python script as environment variables
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USERNAME: ${{ secrets.DB_USERNAME }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # The Python script now handles all logic and logging.
          # We just run it and pipe its output to a log file.
          echo "::group::Crawler Log Output"
          python -m hubert.data_ingestion.huber_crawler.main | tee crawler_output.txt
          echo "::endgroup::"

      - name: Upload logs and metrics as artifacts
        if: always() # Run this step even if the crawler fails
        uses: actions/upload-artifact@v4
        with:
          name: crawler-run-logs-${{ github.run_id }}
          path: |
            crawler_output.txt
            crawler_metrics.json
          retention-days: 7 # Keep logs for 7 days

      - name: Create job summary from metrics file
        if: success() && steps.crawler.outcome == 'success'
        run: |
          # Read metrics directly from the JSON file to create a clean summary
          TOTAL_RUNTIME=$(jq -r '.total_runtime_seconds // 0' crawler_metrics.json)
          TOTAL_URLS=$(jq -r '.total_urls_found // 0' crawler_metrics.json)
          NEW_URLS=$(jq -r '.new_urls // 0' crawler_metrics.json)
          REMOVED_URLS=$(jq -r '.removed_urls // 0' crawler_metrics.json)
          ERRORS=$(jq -r '.errors // 0' crawler_metrics.json)

          echo "## Sitemap Crawler Run Summary" >> $GITHUB_STEP_SUMMARY
          echo "Run completed at: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|---|---|" >> $GITHUB_STEP_SUMMARY
          echo "| Total runtime | **${TOTAL_RUNTIME%.*} seconds** |" >> $GITHUB_STEP_SUMMARY
          echo "| Total URLs found | **$TOTAL_URLS** |" >> $GITHUB_STEP_SUMMARY
          echo "| New/Updated URLs | **$NEW_URLS** |" >> $GITHUB_STEP_SUMMARY
          echo "| Removed URLs | **$REMOVED_URLS** |" >> $GITHUB_STEP_SUMMARY
          echo "| Errors | **$ERRORS** |" >> $GITHUB_STEP_SUMMARY
          
      - name: Commit metrics history
        if: success() # Only commit if the job was successful
        run: |
          # Create a directory for today's metrics
          METRICS_DIR="metrics/$(date +'%Y-%m-%d')"
          mkdir -p $METRICS_DIR
          
          # Move the generated metrics and logs into the directory
          mv crawler_metrics.json $METRICS_DIR/
          mv crawler_output.txt $METRICS_DIR/

          # Update historical metrics file
          if [ ! -f "metrics_history.csv" ]; then
            echo "date,total_runtime,total_urls,new_urls,removed_urls,errors" > metrics_history.csv
          fi
          
          # Append today's summary metrics
          TODAY=$(date +"%Y-%m-%d")
          RUNTIME=$(jq -r '.total_runtime_seconds' $METRICS_DIR/crawler_metrics.json)
          URLS=$(jq -r '.total_urls_found' $METRICS_DIR/crawler_metrics.json)
          NEW=$(jq -r '.new_urls' $METRICS_DIR/crawler_metrics.json)
          REMOVED=$(jq -r '.removed_urls' $METRICS_DIR/crawler_metrics.json)
          ERRORS=$(jq -r '.errors' $METRICS_DIR/crawler_metrics.json)
          echo "$TODAY,${RUNTIME},${URLS},${NEW},${REMOVED},${ERRORS}" >> metrics_history.csv

          # Commit the changes back to the repository
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add metrics/ metrics_history.csv
          # Only commit and push if there are actual changes
          git diff --quiet && git diff --staged --quiet || (git commit -m "ðŸ“Š Update crawler metrics for $TODAY" && git push)
