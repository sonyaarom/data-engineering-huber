name: Sitemap Crawler

on:
  schedule:
    # Run daily at 3:00 AM UTC
    - cron: '0 3 * * *'
  # Optional: Allow manual triggering
  workflow_dispatch:

permissions:
  contents: write

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Ensure additional dependencies needed for crawling are installed
          pip install psycopg2-binary requests beautifulsoup4
          
      - name: Run sitemap crawler
        id: crawler
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USERNAME: ${{ secrets.DB_USERNAME }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # Set PYTHONPATH to include the project root
          export PYTHONPATH=$GITHUB_WORKSPACE:$PYTHONPATH
          # Run the crawler script and capture start time
          echo "::group::Crawler Log Output"
          START_TIME=$(date +%s)
          python -m hubert.data_ingestion.huber_crawler.main | tee crawler_output.txt
          CRAWLER_EXIT_CODE=$?
          END_TIME=$(date +%s)
          echo "::endgroup::"
          
          # Set default values for metrics
          echo "TOTAL_RUNTIME=$((END_TIME - START_TIME))" >> $GITHUB_ENV
          echo "TOTAL_URLS=0" >> $GITHUB_ENV
          echo "NEW_URLS=0" >> $GITHUB_ENV
          echo "UPDATED_URLS=0" >> $GITHUB_ENV
          echo "REMOVED_URLS=0" >> $GITHUB_ENV
          echo "ERRORS=0" >> $GITHUB_ENV
          
          # Extract metrics from the output if file exists
          if [ -f "crawler_metrics.json" ]; then
            TOTAL_RUNTIME=$(jq -r '.total_runtime_seconds // "not found"' crawler_metrics.json)
            if [ "$TOTAL_RUNTIME" != "not found" ]; then
              echo "TOTAL_RUNTIME=$TOTAL_RUNTIME" >> $GITHUB_ENV
            fi
            
            TOTAL_URLS=$(jq -r '.total_urls_found // "0"' crawler_metrics.json)
            echo "TOTAL_URLS=$TOTAL_URLS" >> $GITHUB_ENV
            
            NEW_URLS=$(jq -r '.new_urls // "0"' crawler_metrics.json)
            echo "NEW_URLS=$NEW_URLS" >> $GITHUB_ENV
            
            UPDATED_URLS=$(jq -r '.updated_urls // "0"' crawler_metrics.json)
            echo "UPDATED_URLS=$UPDATED_URLS" >> $GITHUB_ENV
            
            REMOVED_URLS=$(jq -r '.removed_urls // "0"' crawler_metrics.json)
            echo "REMOVED_URLS=$REMOVED_URLS" >> $GITHUB_ENV
            
            ERRORS=$(jq -r '.errors // "0"' crawler_metrics.json)
            echo "ERRORS=$ERRORS" >> $GITHUB_ENV
          else
            echo "No metrics file found. Using default values."
            if [ $CRAWLER_EXIT_CODE -ne 0 ]; then
              echo "ERRORS=1" >> $GITHUB_ENV
            fi
          fi

      - name: Create job summary
        run: |
          echo "## Sitemap Crawler Run Summary" >> $GITHUB_STEP_SUMMARY
          echo "Run completed at: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Metrics" >> $GITHUB_STEP_SUMMARY
          echo "- **Total runtime:** ${{ env.TOTAL_RUNTIME }} seconds" >> $GITHUB_STEP_SUMMARY
          echo "- **Total URLs found:** ${{ env.TOTAL_URLS }}" >> $GITHUB_STEP_SUMMARY
          echo "- **New URLs:** ${{ env.NEW_URLS }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Updated URLs:** ${{ env.UPDATED_URLS }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Removed URLs:** ${{ env.REMOVED_URLS }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Errors:** ${{ env.ERRORS }}" >> $GITHUB_STEP_SUMMARY
          
      # Store metrics files if they exist
      - name: Save metrics to repository
        run: |
          # Create a metrics directory if it doesn't exist
          mkdir -p metrics/$(date +"%Y-%m-%d")
          
          # Create a summary file with the extracted metrics
          echo "{
            \"date\": \"$(date +"%Y-%m-%d")\",
            \"total_runtime\": \"${{ env.TOTAL_RUNTIME }}\",
            \"total_urls\": \"${{ env.TOTAL_URLS }}\",
            \"new_urls\": \"${{ env.NEW_URLS }}\",
            \"updated_urls\": \"${{ env.UPDATED_URLS }}\",
            \"removed_urls\": \"${{ env.REMOVED_URLS }}\",
            \"errors\": \"${{ env.ERRORS }}\"
          }" > metrics/$(date +"%Y-%m-%d")/summary.json
          
          # Copy crawler output if it exists
          if [ -f "crawler_output.txt" ]; then
            cp crawler_output.txt metrics/$(date +"%Y-%m-%d")/
          else
            echo "No crawler output file found." > metrics/$(date +"%Y-%m-%d")/crawler_output.txt
          fi
          
          # Copy crawler metrics if it exists
          if [ -f "crawler_metrics.json" ]; then
            cp crawler_metrics.json metrics/$(date +"%Y-%m-%d")/
          fi

      - name: Update historical metrics
        run: |
          # Create historical metrics file if it doesn't exist
          if [ ! -f "metrics_history.csv" ]; then
            echo "date,total_runtime,total_urls,new_urls,updated_urls,removed_urls,errors" > metrics_history.csv
          fi
          
          # Append today's metrics
          TODAY=$(date +"%Y-%m-%d")
          echo "$TODAY,${{ env.TOTAL_RUNTIME }},${{ env.TOTAL_URLS }},${{ env.NEW_URLS }},${{ env.UPDATED_URLS }},${{ env.REMOVED_URLS }},${{ env.ERRORS }}" >> metrics_history.csv
          
      - name: Commit metrics history and results
        run: |
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'actions@github.com'
          git add metrics_history.csv
          git add metrics/
          git diff --quiet && git diff --staged --quiet || (git commit -m "Update metrics [skip ci]" && git push)
