name: Sitemap Crawler

on:
  schedule:
    # Run daily at 3:00 AM UTC
    - cron: '0 3 * * *'
  # Optional: Allow manual triggering
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Ensure additional dependencies needed for crawling are installed
          pip install psycopg2-binary requests beautifulsoup4
          
      - name: Run sitemap crawler
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USERNAME: ${{ secrets.DB_USERNAME }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          # Set PYTHONPATH to include the project root
          export PYTHONPATH=$GITHUB_WORKSPACE:$PYTHONPATH
          # Run the crawler script
          python -m hubert.data_ingestion.huber_crawler.main